<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>My Portfolio</title>
  <link rel="stylesheet" href="assets/css/style.css">
</head>
<body>
  <!-- Header -->
  <header class="header">
    <h1 class="logo">My Portfolio</h1>
    <nav class="navbar">
      <ul>
        <li><button class="tab-link active" data-tab="about">About</button></li>
        <li><button class="tab-link" data-tab="education">Education</button></li>
        <li><button class="tab-link" data-tab="research">Research</button></li>
        <li><button class="tab-link" data-tab="projects">Projects</button></li>
        <li><button class="tab-link" data-tab="skills">Skills</button></li>
        <li><button class="tab-link" data-tab="contact">Contact</button></li>
      </ul>
    </nav>
    <button id="theme-toggle" class="theme-toggle">ðŸŒ™</button>
  </header>

  <!-- Main Content -->
  <main>
    <section id="about" class="tab-content active">
      <div class="hero-container">
        <img src="assets/images/Snehit_ProfilePhoto.jpg" alt="Profile Picture" class="profile-pic">
        <div class="hero-text">
          <h2>About Me</h2>
          <p>Hello, I'm Snehit, a PhD researcher passionate about building intelligent systems to understand human emotions in the real-world environment.<br><br><br>I enjoy exploring-
            <ul>
              <li>Deep learning models for speech and audio representation.</li>
              <li>Multimodal Transformers</li>
              <li>Modality Fusion approaches</li>
              <li>Visualising Emmbedings</li>
            </ul>
        </p>
          <div class="hero-buttons">
            <button class="btn tab-link" data-tab="contact">Contact Me</button>
            <button class="btn tab-link" data-tab="projects">View Projects</button>
          </div>
        </div>
      </div>
    </section>


    <section id="education" class="tab-content">
      <h2>Education<br><br></h2>
      <div class="education-item">
        <h3>PhD in Electrical Engineering (Ongoing) </h3>
        <div class="edu-details">
          <span class="college">National Tsing Hua University</span>
          <span class="location">Hsinchu, Taiwan</span>
        </div>
      </div>
    
      <div class="education-item">
        <h3>Masterâ€™s in Instrumentation and Signal Processing</h3>
        <div class="edu-details">
          <span class="college">Indian Institute of Technology Roorkee</span>
          <span class="location">Roorkee, India</span>
        </div>
      </div>
    
      <div class="education-item">
        <h3>Bachelorâ€™s in Instrumentation Engineering</h3>
        <div class="edu-details">
          <span class="college">Government College of Engineering</span>
          <span class="location">Chandrapur, India</span>
        </div>
      </div>
    </section>


    <section id="research" class="tab-content">
      <h2>Research & Publications</h2>
    
      <div class="publication-box">
        <div class="pub-header">
          <a href="https://dcase.community/documents/workshop2023/proceedings/DCASE2023Workshop_Chunarkar_49.pdf" target="_blank" class="pub-title">
            STELIN-US: A SPATIO-TEMPORALLY LINKED NEIGHBORHOOD URBAN SOUND DATABASE
          </a>
          <p class="pub-authors">Snehit Chunarkar, Bo-Hao Su, Chi-Chun Lee â€” DCASE 2023</p>
        </div>
        <div class="pub-abstract">
          <p>
            Automated acoustic understanding, e.g., sound event detection and acoustic scene recognition, is an important research direction 
            enabling numerous modern technologies. Although there is a wealth of corpora, most, if not all, include acoustic samples of 
            scenes/events in isolation without considering their interconnectivity with locations nearby in a neighborhood. Within a connected 
            neighborhood, the temporal continuity and regional limitation (sound-location dependency) at distinct locations creates noniid acoustics 
            samples at each site across spatial-temporal dimensions. To our best knowledge, none of the previous data sources takes on this particular angle. 
            In this work, we present a novel dataset, the Spatio-temporally Linked Neighborhood Urban Sound (STeLiN-US) database. The dataset is semi-synthesized, 
            that is, each sample is generated by leveraging diverse sets of real urban sounds with crawled information of real-world user behaviors over time. 
            This method helps create a realistic large-scale dataset, and we further evaluate it through perceptual listening tests. This neighborhood-based 
            data generation opens up novel opportunities to advance user-centered applications with automated acoustic understanding. For example, to develop 
            real-world technology to model a userâ€™s speech data over a day, one can imagine utilizing this dataset as the userâ€™s speech samples would modulate by 
            diverse sources of acoustics surrounding linked across sites and temporally by natural behavior dynamics at each location over time.
          </p>
        </div>
      </div>
    
      <div class="publication-box">
        <div class="pub-header">
          <a href="https://ieeexplore.ieee.org/abstract/document/9707959/" target="_blank" class="pub-title">
            Mixed Language Separation Using Deep Neural Network
          </a>
          <p class="pub-authors">Snehit Chunarkar, Samba Raju Chiluveru, Manoj Tripathy â€” ICEECCOT 2021</p>
        </div>
        <div class="pub-abstract">
          <p>
            With multiple languages spoken in the world by different groups of people, we may encounter mixed language
            speech to hear, especially while vlogging in a different country or during interviews with voice dubbing. The appropriate
            language speech audio can be extracted from a mixed one using a separation mechanism. This paper proposes a DNN
            model to perform such a language separation task. Different features like Mel Frequency Cepstrum Coefficient (MFCC),
            Power Spectrum, and Relative Spectral Transformed Perceptual Linear Prediction coefficient (RASTA-PLP) are
            extracted from the mixed language speech as the input to the DNN. For the training target, the Short-Time Fourier
            Transform (STFT) Spectral Mask is considered. To understand the improvement on the speech, the
            processed speech is then evaluated for its intelligibility and quality. Here Short-time Objective Intelligibility (STOI) and
            Perceptual Evaluation of Speech Quality (PESQ) scores are used to compare the Intelligibility and Quality of the separated
            language speech signal processed by the DNN. It can be observed from the results that the language separated audio
            using a trained DNN model has shown improved Intelligibility and Quality.
          </p>
        </div>
      </div>
    
    </section>


    <section id="projects" class="tab-content">
      <h2>Projects</h2>
      <p>Highlight GitHub repos, Kaggle work, HuggingFace Spaces, demos.</p>
    </section>

    <section id="skills" class="tab-content">
      <h2>Skills</h2>
      <ul>
        <li>Python, PyTorch, TensorFlow</li>
        <li>Transformers, WavLM, CLAP</li>
        <li>Data Processing, Visualization</li>
      </ul>
    </section>

    <section id="contact" class="tab-content">
      <h2>Contact</h2>
      <p>Email: yourname@email.com</p>
      <p>LinkedIn | GitHub | Google Scholar</p>
    </section>
  </main>

  <script src="assets/js/script.js"></script>
</body>
</html>
