<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>My Portfolio</title>
  <link rel="stylesheet" href="assets/css/style.css">
</head>
<body>
  <!-- Header -->
  <header class="header">
    <h1 class="logo">My Portfolio</h1>
    <nav class="navbar">
      <ul>
        <li><button class="tab-link active" data-tab="about">About</button></li>
        <li><button class="tab-link" data-tab="education">Education</button></li>
        <li><button class="tab-link" data-tab="research">Research</button></li>
        <li><button class="tab-link" data-tab="projects">Projects</button></li>
        <li><button class="tab-link" data-tab="skills">Skills</button></li>
        <li><button class="tab-link" data-tab="contact">Contact</button></li>
      </ul>
    </nav>
    <button id="theme-toggle" class="theme-toggle">ðŸ”…</button>
  </header>

  <!-- Main Content -->
  <main>
    <section id="about" class="tab-content active">
      <div class="hero-container">
        <img src="assets/images/Snehit_ProfilePhoto.jpg" alt="Profile Picture" class="profile-pic">
        <div class="hero-text">
          <h2>About Me</h2>
          <p class="intro-text">
            Hi, Iâ€™m <strong>Snehit Chunarkar</strong>, a PhD researcher exploring the fascinating world of 
            <strong>machine learning, speech, and emotion understanding</strong>. 
            My work focuses on building intelligent systems that can perceive and interpret 
            human emotions through sound and language in natural, real-world settings.
          </p>
    
          <div class="explore-section">
            <h4>Iâ€™m particularly interested in:</h4>
            <ul class="explore-list">
              <li>Speech & audio representation learning</li>
              <li>Multimodal Transformers and cross-modal fusion</li>
              <li>Emotion reasoning and context-aware AI</li>
              <li>Visualization of embeddings & interpretability</li>
            </ul>
          </div>
    
          <div class="hero-buttons">
            <button class="btn tab-link" data-tab="contact">Contact Me</button>
            <button class="btn tab-link" data-tab="projects">View Projects</button>
          </div>
        </div>
      </div>
    </section>



    <section id="education" class="tab-content">
      <h2>Education</h2>
      <div class="education-item">
        <h3>PhD in Electrical Engineering (Ongoing) </h3>
        <div class="edu-details">
          <span class="college">National Tsing Hua University</span>
          <span class="location">Hsinchu, Taiwan</span>
        </div>
      </div>
    
      <div class="education-item">
        <h3>Masterâ€™s in Instrumentation and Signal Processing</h3>
        <div class="edu-details">
          <span class="college">Indian Institute of Technology Roorkee</span>
          <span class="location">Roorkee, India</span>
        </div>
      </div>
    
      <div class="education-item">
        <h3>Bachelorâ€™s in Instrumentation Engineering</h3>
        <div class="edu-details">
          <span class="college">Government College of Engineering</span>
          <span class="location">Chandrapur, India</span>
        </div>
      </div>
    </section>


    <section id="research" class="tab-content">
      <h2>Research & Publications</h2>
    
      <div class="publication-box">
        <div class="pub-header">
          <a href="https://dcase.community/documents/workshop2023/proceedings/DCASE2023Workshop_Chunarkar_49.pdf" target="_blank" class="pub-title">
            STELIN-US: A SPATIO-TEMPORALLY LINKED NEIGHBORHOOD URBAN SOUND DATABASE
          </a>
          <p class="pub-authors">
            <span class="authors"><strong>Snehit Chunarkar</strong>strong>, Bo-Hao Su, Chi-Chun Lee</span>
            <span class="conference">DCASE 2023 (Tampere, Finland)</span>
          </p>
        </div>
        <div class="pub-abstract">
          <p>
            Automated acoustic understanding, e.g., sound event detection and acoustic scene recognition, is an important research direction 
            enabling numerous modern technologies. Although there is a wealth of corpora, most, if not all, include acoustic samples of 
            scenes/events in isolation without considering their interconnectivity with locations nearby in a neighborhood. Within a connected 
            neighborhood, the temporal continuity and regional limitation (sound-location dependency) at distinct locations creates noniid acoustics 
            samples at each site across spatial-temporal dimensions. To our best knowledge, none of the previous data sources takes on this particular angle. 
            In this work, we present a novel dataset, the Spatio-temporally Linked Neighborhood Urban Sound (STeLiN-US) database. The dataset is semi-synthesized, 
            that is, each sample is generated by leveraging diverse sets of real urban sounds with crawled information of real-world user behaviors over time. 
            This method helps create a realistic large-scale dataset, and we further evaluate it through perceptual listening tests. This neighborhood-based 
            data generation opens up novel opportunities to advance user-centered applications with automated acoustic understanding. For example, to develop 
            real-world technology to model a userâ€™s speech data over a day, one can imagine utilizing this dataset as the userâ€™s speech samples would modulate by 
            diverse sources of acoustics surrounding linked across sites and temporally by natural behavior dynamics at each location over time.
          </p>
        </div>
      </div>
    
      <div class="publication-box">
        <div class="pub-header">
          <a href="https://ieeexplore.ieee.org/abstract/document/9707959/" target="_blank" class="pub-title">
            Mixed Language Separation Using Deep Neural Network
          </a>
          <p class="pub-authors">
            <span class="authors"><strong>Snehit Chunarkar</strong>strong>, Samba Raju Chiluveru, Manoj Tripathy</span>
            <span class="conference">ICEECCOT 2021 (Mysuru, India)</span>
          </p>
        </div>
        <div class="pub-abstract">
          <p>
            With multiple languages spoken in the world by different groups of people, we may encounter mixed language
            speech to hear, especially while vlogging in a different country or during interviews with voice dubbing. The appropriate
            language speech audio can be extracted from a mixed one using a separation mechanism. This paper proposes a DNN
            model to perform such a language separation task. Different features like Mel Frequency Cepstrum Coefficient (MFCC),
            Power Spectrum, and Relative Spectral Transformed Perceptual Linear Prediction coefficient (RASTA-PLP) are
            extracted from the mixed language speech as the input to the DNN. For the training target, the Short-Time Fourier
            Transform (STFT) Spectral Mask is considered. To understand the improvement on the speech, the
            processed speech is then evaluated for its intelligibility and quality. Here Short-time Objective Intelligibility (STOI) and
            Perceptual Evaluation of Speech Quality (PESQ) scores are used to compare the Intelligibility and Quality of the separated
            language speech signal processed by the DNN. It can be observed from the results that the language separated audio
            using a trained DNN model has shown improved Intelligibility and Quality.
          </p>
        </div>
      </div>

      <div class="publication-box">
        <div class="pub-header">
          <a href="https://ieeexplore.ieee.org/abstract/document/10762730" target="_blank" class="pub-title">
            Efficient Hardware Implementation of Nonlinear Activation Function For Inference Model
          </a>
          <p class="pub-authors">
            <span class="authors"><strong>Snehit Chunarkar</strong>strong>, Samba Raju Chiluveru</span>
            <span class="conference">ISOCC 2024 (Sapporo, Japan)</span>
          </p>
        </div>
        <div class="pub-abstract">
          <p>
            This paper presents a novel piecewise linear (PWL) approximation method for designing a highly precise nonlinear activation function tailored 
            for hardware implementation of inference models. It is developed as an Adaptive Step-Size-based Recursive Algorithm (ASRA) method, incorporating 
            the maximum allowable error (Ïµ) as an input parameter. PWL functions are realized with minimal computational overhead, utilizing only addition 
            operations and coefficient memory, thus avoiding multiplications. With fewer resources, the proposed method allows for the accurate approximation 
            of nonlinear functions. The hardware implementation uses a Synopsys Design Compiler with a TSMC 90-nm library. Performance comparison in terms of area, 
            delay, and power consumption demonstrates the effectiveness of the proposed approach.
          </p>
        </div>
      </div>
    
    </section>


    <section id="projects" class="tab-content">
      <h2>Projects</h2>
      <p>Highlight GitHub repos, Kaggle work, HuggingFace Spaces, demos.</p>
    </section>

    <section id="skills" class="tab-content">
      <h2>Skills</h2>
      <ul>
        <li>Python, PyTorch, TensorFlow</li>
        <li>Transformers, WavLM, CLAP</li>
        <li>Data Processing, Visualization</li>
      </ul>
    </section>

    <section id="contact" class="tab-content">
      <h2>Contact</h2>
      <p>Email: yourname@email.com</p>
      <p>LinkedIn | GitHub | Google Scholar</p>
    </section>
  </main>

  <script src="assets/js/script.js"></script>
</body>
</html>
